---
title: 大模型学习笔记（一）——综述笔记
date: 2025-08-12 10:00:00
categories: [笔记, LLM]
tags: [LLM, 大模型, Transformer, Attention, Tokenizer, Embedding, 综述]
cover: https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E7%8C%8E%E5%BE%B7%E5%A4%A7%E6%A1%A5&%E5%B9%BF%E5%B7%9E%E5%A1%94%E2%80%94%E2%80%94%E5%B9%BF%E5%B7%9E.jpg
series: 大模型学习笔记
katex: true
---

## 综述《Large Language Models: A Survey》粗读

### 摘要

本文主要内容包括：
1. 阐述当下主流的三个大模型家族（**GPT**,**LLaMa**,**PaLM**)的**特点**、**贡献**和**局限性**。
2. 总览了**制作**、**增强**大模型的技术。
3. 介绍了用于大模型**训练**、**微调**、**评估**的数据集。
4. 介绍了大模型**评估指标**，并对比了各类大模型在一些代表性的**benchmark**上的表现。
5. 讨论了一些公开的**挑战**和**未来的研究方向**。

### 图表

![大模型能力](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B.png)
规模较小的模型无法拥有Emergent Abilities（涌现能力），它不是线性增长的，而是在模型的参数量达到某一个临界值后突然“获得”的。
涌现能力包括上下文学习、指令跟随、多步推理。

![各类热门（2024）大模型](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E5%90%84%E7%B1%BB%E7%83%AD%E9%97%A8%EF%BC%882024%EF%BC%89%E5%A4%A7%E6%A8%A1%E5%9E%8B.png)
可以看到使用频率最高的训练数据库是[Common Crawl](https://commoncrawl.org/)、代码数据集（GitHub、Code datasets、SlimPajama 等）、学术/科学数据集（Arxiv、StackExchange、DocBank）。

![BERT的预训练和微调流程](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/BERT%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B.png)
通过预训练得到一个学会通用语言表示的模型，然后通过微调来解决具体的任务。
预训练的流程：
1. 以掩盖部分词的句子对的形式输入，让大模型预测掩盖的是什么，并且输出下一句预测。
2. E是嵌入表示，是每个 token 的嵌入向量是三个部分相加的：Token Embedding（词嵌入）、Segment Embedding（句子 A/B 的区分向量）、Position Embedding（位置编码）。
3. T是上下文表示（BERT真正“理解”后的词表示，可用于下游任务），是经过多层Transformer编码之后的输出向量，借由双向注意力机制整合了句子里其它所有词的语义信息。
4. 重点是理解掩盖词元(Masked Language Model - MLM)这种方式的意义。我们无法直接告诉模型一句话是什么意思，所以设计这样的训练方式，模型要去预测被掩盖的词，它就一定需要理解上下文。
5. 除了掩盖词元，在ELECTRA里引入了替换词元(Replaced Token Detection - RTD)，它替换了部分词，让模型去判断每一个词是原装的还是被替换的。

`[CLS]` 和 `[SEP]` 这两个简写来自 **classification** 和 **separator** 的缩写：
* `[CLS]` 表示“分类专用标记”，在 BERT 中放在输入序列开头，用它的输出向量做句子级任务（分类、回归等）的特征表示。
* `[SEP]` 表示“分隔符标记”，用于分隔两个句子或标记序列的结束位置。

微调的流程：
复杂、暂略。

![XLM](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/XLM.png)
* XLM的MLM与BERT的MLM的区别是，它不是成对句子输入，而是流式的输入，目标都是预测被掩盖的词。
* XLM的TLM部分需要再不同语言的表征间对齐(alignment)。

![UniLM](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/UniLM.png)
* UniLM的特点是使用一种模型结构，支持了三种模式：双向语言模型(Bidirectional LM)、单向语言模型(Left-to-right LM)、序列到序列语言模型(Seq-Seq LM)。
* 实现方式是使用注意力遮罩，限制计算一个词的表示时，它能看到哪些其它的词，方式如图所示。

![早期GPT的宏观结构和对四种下游任务的微调方法](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E6%97%A9%E6%9C%9FGPT%E7%9A%84%E5%AE%8F%E8%A7%82%E7%BB%93%E6%9E%84%E5%92%8C%E5%AF%B9%E5%9B%9B%E7%A7%8D%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95.png)
* 左边描述了GPT的宏观结构，输入经过文本编码和位置编码，进入12层的Transformer编码器，最后得到文本预测和任务分类的结果。
* 右边是四种任务的微调方法：文本分类(Classification)、文本蕴含(Entailment)、文本相似度(Similarity)和多项选择(Multiple Choice)。
* 这里通过构造不同的输入格式、Transformer模块的数量和组织形式，来构造不同的任务。

![示例、任务描述对不同规模的大模型完成任务的影响](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E7%A4%BA%E4%BE%8B%E3%80%81%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0%E5%AF%B9%E4%B8%8D%E5%90%8C%E8%A7%84%E6%A8%A1%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%8C%E6%88%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BD%B1%E5%93%8D.png)
* **误区阐述：** 我认为这幅图最有价值的地方并非是说明了模型越大从上下文中学习的能力就越强，这一点可能是正确的，这也是所谓涌现的一种表现。
* **提示词是无关因素：** 通过对比实线与虚线，可以发现三色的一个共同点，在示例足够多的时候，有无提示词变成了一个无关因素，示例非常有限时，提示词才会有作用。
* **提示词和示例是交叉并行的：** 我想这很好理解，少量示例描述任务范围太大，示例充分则可以清晰地定义任务的边界。提示词和示例是两条并行的路，终点都是为了向大模型描述任务。
* **提示词和示例的深层内涵：** 大语言模型理解文字任务描述的过程是将文字抽象化成为某种任务表示，理解示例也是如此，所以关键问题是这种转化的质量和速度。当前对 ICL(In-context Learning) 的几种理论对此有不同的解释：
  * 贝叶斯推断理论（Bayesian Inference Hypothesis）：模型在上下文中形成对“潜在任务”的后验分布，ICL 则是在给定样本条件下的分布更新。
  * 线性/核回归视角：Transformer 在上下文内执行一种近似回归/检索加权。
  * 元学习理论（Meta-Learning Hypothesis）：模型学会了读示例→拟合小问题的“内在优化器”，ICL 只是把这个能力激活出来。
  * 模型记忆检索理论（Retrieval / Pattern Matching Hypothesis）：模型在预训练阶段记住了大量“类似任务的模式”，ICL 时只是在检索和组合它学过的模式。
  * 模拟微调理论（Implicit Fine-Tuning Hypothesis）：ICL 的本质是隐式的梯度下降，模型在推理时，会用前面示例去更新内部的“激活状态”，等价于在临时微调参数（虽然物理参数没变）。
* **困难原因分析：** LLM之所以无法从可能在人类看来清晰的规律或者描述中准确学习到精准的任务表示，我认为有以下几种原因：
  * 抽象的任务表示的可能性太多，模型难以精准定位到目标任务，具体来说有以下几类原因：
    * 先验错配：预训练的下一个词目标未必鼓励精确执行算法式规则，模型会选择更便宜的启发式。
    * 分布与格式偏差：示例格式、顺序、位置对注意力有强影响；噪声或冲突示例会让后验发散。
    * 上下文容量与干扰：长上下文中远端示例衰减，易被近端干扰覆盖（recency bias）。
    * 解码策略影响：温度、惩罚项会放大/缩小启发式偏好，导致“看起来会”但输出不稳。
    * 指令对齐程度：没有做过指令调优/对齐的基座模型，指令可读但不一定“优先服从”。
  * 模型可以表达的任务表示的种类有限，没有覆盖目标任务。
* **解决路线：** 我认为可以从纯自然科学的角度出发，研究大模型如何表示任务，再根据成熟的理论定制化设计模型的任务表示能力。
  * 自上而下（工程对齐）：指令微调、合成数据覆盖、格式鲁棒训练、思维链/检验器、工具调用与规划模块化，把“任务表示”外显化并可控。
  * 自下而上（机制研究）：可解释性分析注意力回路、软提示/前缀向量作为“任务嵌入”、在合成分布上验证 ICL 的内在算法与失效模式。
  * 相关领域：**大模型可解释性（Interpretability）和神经符号（Neuro-Symbolic AI）**

![RLHF的流程](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/RLHF%E7%9A%84%E6%B5%81%E7%A8%8B.png)
1. 监督微调（SFT）：使用人工标注、设计的数据进行一次粗对齐。
    公式： 
    $$
    \min_{\theta} \; L_{\text{SFT}}(\theta) 
    = - \mathbb{E}_{(x, y) \sim D_{\text{SFT}}} \left[ \log \pi_\theta(y \mid x) \right]
    $$
    **参数说明**：
    - $\pi_\theta(y \mid x)$：策略模型的条件概率  
    - $D_{\text{SFT}}$：人工标注数据集  
    - $\theta$：策略模型参数  
2. 奖励模型（RM）：对于同一个prompt让模型生成多条回答，人工排序，用以训练一个模型输出的打分器。
    $$
    \min_{\phi} \; L_{\text{RM}}(\phi) 
    = - \mathbb{E}_{(x, y_{\text{优}}, y_{\text{劣}}) \sim D_{\text{RM}}} 
    \left[ \log \sigma \left( r_\phi(x, y_{\text{优}}) - r_\phi(x, y_{\text{劣}}) \right) \right]
    $$

    **参数说明**：
    - $r_\phi(x, y)$：奖励模型的实数分输出  
    - $y_{\text{优}}, y_{\text{劣}}$：人类标注的优劣答案  
    - $\sigma(z) = \frac{1}{1 + e^{-z}}$：Sigmoid 函数  
    - $D_{\text{RM}}$：人类偏好排序数据集  
    - $\phi$：奖励模型参数  
3. 强化学习（RL）：使用PPO算法，根据打分器的结果调整模型参数，迭代循环，直到足够符合人类偏好。
    公式：  
    $$
    \max_{\theta} \; \mathbb{E}_{x \sim D, \; y \sim \pi_\theta(\cdot \mid x)}
    \left[ r_\phi(x, y) - \beta \; \mathrm{KL}\big(\pi_\theta(\cdot \mid x) \,\|\, \pi_{\text{ref}}(\cdot \mid x)\big) \right]
    $$

    **参数说明**：
    - $\pi_\theta$：当前策略模型  
    - $\pi_{\text{ref}}$：参考策略（通常是 SFT 模型或早期快照）  
    - $r_\phi(x, y)$：奖励模型的分数  
    - $\beta$：KL 惩罚系数  
    - $\mathrm{KL}(p \| q)$：KL 散度，衡量分布差异  

直觉上这个过程不精准，因为RM不是精准的，它的误差在RL那会被放大。

![GPT-4为裁判的大模型质量评估](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/GPT-4%E4%B8%BA%E8%A3%81%E5%88%A4%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0.png)
这个评估的流程是：
1. 准备一批多样化的测试问题。
2. 让每个模型输出回答。
3. 让GPT-4作为裁判，对每个模型的回答进行评估，直接输出一个分数，或者做配对比较，看哪个更好，赢得一分，输扣一分。

![三种微调/使用范式](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E4%B8%89%E7%A7%8D%E5%BE%AE%E8%B0%83%E8%8C%83%E5%BC%8F.png)
1. Pretrain–finetune(预训练 + 微调)：在某个具体任务(task A)上进行微调。最终模型专门用于该任务。
2. Prompting(提示学习)：不做进一步微调；在预测时用提示(prompts)或少量样例(few-shot learning)提高在某个任务(task A)上的表现。
3. Instruction Tuning(指令微调)：用多种不同类型的任务数据，用自然语言指令来微调模型，从而让模型学会“听从指令”这个元能力。

![Retro架构](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/Retro%E6%9E%B6%E6%9E%84.png)
1. 把输入序列分块（chunking），每个chunk作为一个处理单元。
2. 从外部数据库检索相关片段(retrieval)，找到“邻居”序列作为参考。
3. 通过交叉注意力(chunked cross-attention, CCA)融合检索到的邻居信息。
4. 继续像普通Transformer块一样处理，生成预测。

问题：
1. Retrieval数据库如何构建？
2. CCA过程多大程度上有效？为何有效？
3. CCA的子过程CA做了什么事情？

![GLaM模型的结构](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/GLaM%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84.png)
上半部分是传统的Transformer模型，下半部分是MoE层。
通过一个Gating(门控)模块选择决定激活哪些专家(FFN)。

![Sparrow pipeline](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/Sparrow%20pipeline.png)
人类的反馈有两种形式：
1. 偏好响应，模型生成多种答案，人类评估员选择他们认为最好的。
2. 对抗性探测，人类评估员扮演敌手(adversary)用各种方式诱导模型违反规则，以此找出模型的弱电和漏洞。

![UL2](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/UL2.png)
1. S-denoiser (序贯去噪 / Sequential Denoising): 这本质上就是GPT系列的训练方式，即“下一个词预测”。给模型一段前文（prefix），让它续写后面的内容。这个任务特别擅长训练模型的生成能力和上下文学习 (In-context Learning) 能力。
2. R-denoiser (常规去噪 / Regular Denoising): 这类似于T5模型的训练方式。它会随机遮盖文本中一些较短的片段，让模型去填空。这个任务对于训练模型的语言理解能力非常有效。
3. X-denoiser (极限去噪 / eXtreme Denoising): 这是R-denoiser的“困难模式”。它会遮盖掉非常长的文本片段，或者以非常高的比例破坏原文。这迫使模型必须依赖更长距离的上下文信息和世界知识才能恢复原文，从而极大地锻炼了模型的长程推理能力和知识储备。

![BLOOM结构](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/BLOOM%E7%BB%93%E6%9E%84.png)
1. 左：整体结构，LN是Layer Normalization。
2. 中：解码器内部。
3. 右：多头注意力，核心创新是ALIBI Mask (Attention with Linear Biases)。
  解释：
    * ALIBI是一种根据位置信息为注意力分数添加偏置的方法，抛弃了原本Transformer模型中的位置编码。
    * 右上角为-inf的原因是模型不应该看到未来信息，杜绝信息穿越。
    * k<sub>head</sub>是一个与注意力头相关的常数斜率，每个注意力头都有自己专属的k<sub>head</sub>值，这意味着不同的注意力头可以有不同的距离敏感度。