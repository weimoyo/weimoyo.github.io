---
title: 大模型学习笔记（一）——基础概念
date: 2025-08-12 10:00:00
categories: [笔记, LLM]
tags: [LLM, 大模型, Transformer, Attention, Tokenizer, Embedding, 综述]
series: 大模型学习笔记
---

## Day1——综述《Large Language Models: A Survey》粗读

### 摘要

本文主要内容包括：
1. 阐述当下主流的三个大模型家族（GPT,LLaMa,PaLM)的特点、贡献和局限性。
2. 总览了制作、增强大模型的技术。
3. 介绍了用于大模型训练、微调、评估的数据集。
4. 介绍了大模型评估指标，并对比了各类大模型在一些代表性的benchmark上的表现。
5. 讨论了一些公开的挑战和未来的研究方向。

### 图表

![大模型能力](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B.png)
规模较小的模型无法拥有Emergent Abilities（涌现能力），它不是线性增长的，而是在模型的参数量达到某一个临界值后突然“获得”的。
涌现能力包括上下文学习、指令跟随、多步推理。

![各类热门（2024）大模型](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/%E5%90%84%E7%B1%BB%E7%83%AD%E9%97%A8%EF%BC%882024%EF%BC%89%E5%A4%A7%E6%A8%A1%E5%9E%8B.png)
可以看到使用频率最高的训练数据库是[Common Crawl](https://commoncrawl.org/)、代码数据集（GitHub、Code datasets、SlimPajama 等）、学术/科学数据集（Arxiv、StackExchange、DocBank）。

![BERT的预训练和微调流程](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/BERT%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B.png)
通过预训练得到一个学会通用语言表示的模型，然后通过微调来解决具体的任务。
预训练的流程：
1. 以掩盖部分词的句子对的形式输入，让大模型预测掩盖的是什么，并且输出下一句预测。
2. E是嵌入表示，是每个 token 的嵌入向量是三个部分相加的：Token Embedding（词嵌入）、Segment Embedding（句子 A/B 的区分向量）、Position Embedding（位置编码）。
3. T是上下文表示（BERT真正“理解”后的词表示，可用于下游任务），是经过多层Transformer编码之后的输出向量，借由双向注意力机制整合了句子里其它所有词的语义信息。
4. 重点是理解掩盖词元(Masked Language Model - MLM)这种方式的意义。我们无法直接告诉模型一句话是什么意思，所以设计这样的训练方式，模型要去预测被掩盖的词，它就一定需要理解上下文。
5. 除了掩盖词元，在ELECTRA里引入了替换词元(Replaced Token Detection - RTD)，它替换了部分词，让模型去判断每一个词是原装的还是被替换的。

微调的流程：
复杂、暂略。

![XLM](https://pub-85c6ace1f3f74dfdbd0f332fbb2c2f97.r2.dev/PicGo/XLM.png)
这里的MLM与BERT的MLM的区别是，它不是成对句子输入，而是流式的输入，目标都是预测被掩盖的词。
XLM的TLM部分需要再不同语言的表征间对齐(alignment)。
